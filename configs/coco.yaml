# COCO-specific training configuration for ScribbleDiffusion
model:
  # U-Net Architecture - High-capacity for RTX 3090 24GB VRAM
  unet:
    in_channels: 4  # VAE latent channels
    out_channels: 4
    model_channels: 320  # Standard SD 1.5 base channels
    attention_resolutions: [1, 2, 4, 8]  # Enhanced attention at all scales
    num_res_blocks: 2  # Standard depth for quality
    channel_mult: [1, 2, 4, 4]  # Standard SD progression: 320, 640, 1280, 1280
    num_heads: 8  # Full attention heads
    use_spatial_transformer: true
    transformer_depth: 2  # Deeper transformers for better text conditioning
    context_dim: 768  # Standard CLIP
    use_checkpoint: false  # Disable checkpointing for speed (plenty of VRAM)
    
  # Sketch Conditioning (ControlNet-lite) - Enhanced for high-capacity training
  hint_encoder:
    in_channels: 1  # Binary edge map
    hint_channels: [64, 128, 256, 512]  # Proportional to 320 base channels
    injection_layers: [0, 1, 2, 3]  # Inject at all scales
    injection_method: "add"  # "add" or "film"
    
  # Pretrained Components (frozen)
  vae:
    model_name: "runwayml/stable-diffusion-v1-5"
    subfolder: "vae"
    
  text_encoder:
    model_name: "openai/clip-vit-large-patch14"
    max_length: 77

# Diffusion Settings
diffusion:
  num_train_timesteps: 1000
  beta_schedule: "linear"  # Changed from "cosine" - DDIM doesn't support cosine
  prediction_type: "epsilon"  # "epsilon" or "v_prediction"
  clip_sample: false
  
# Training Configuration
training:
  batch_size: 8  # High-capacity training for RTX 3090
  gradient_accumulation_steps: 4  # Effective batch size of 32  # Increased to maintain effective batch size of 32
  learning_rate: 1e-4
  lr_scheduler: "cosine_with_restarts"
  num_warmup_steps: 1000
  max_train_steps: 100000
  
  # Optimizer
  optimizer: "adamw"
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.999
  epsilon: 1e-8
  
  # Mixed Precision & Memory
  mixed_precision: "fp16"
  gradient_checkpointing: true
  use_ema: true
  ema_decay: 0.9999
  
  # Classifier-Free Guidance Training
  text_drop_prob: 0.1  # Drop text conditioning
  sketch_drop_prob: 0.1  # Drop sketch conditioning
  
  # Loss Configuration
  loss_type: "mse"
  snr_gamma: 5.0  # Signal-to-noise ratio weighting

# COCO Dataset Configuration
data:
  dataset_type: "coco"  # Use COCO dataset
  coco_root: "./data/coco"  # COCO dataset root directory
  image_size: 512  # Higher resolution for RTX 3090
  train_split: train
  validation_split: val
  max_images: 100000  # Use more data with powerful GPU  # Limit dataset size for faster training (remove for full dataset)
  
  # Preprocessing
  random_crop: true
  random_flip: true
  normalize: true
  
  # Edge Detection
  edge_method: "canny"  # "canny", "hed", or "sobel"
  canny_low: 50
  canny_high: 150
  edge_dilation: 1
  edge_erosion: 0
  
  # Edge Augmentations (crucial for robustness)
  edge_jitter: true
  jitter_prob: 0.3
  line_thickness_range: [1, 3]
  gap_prob: 0.1
  gap_size_range: [1, 5]
  
  # COCO-specific settings
  download_coco: true  # Automatically download if not present
  use_captions: true   # Use COCO captions instead of generating descriptions

# Validation & Logging
validation:
  num_validation_images: 32  # More validation samples
  validation_steps: 1000  # More frequent validation
  guidance_scale_text: 7.5
  guidance_scale_sketch: 1.5
  num_inference_steps: 50
  
logging:
  project_name: "scribble-diffusion-coco"
  run_name: null  # Auto-generated if null
  log_with: "tensorboard"  # "wandb", "tensorboard", or "all"
  log_interval: 100
  save_interval: 5000
  
# Paths
paths:
  output_dir: "./outputs/coco_training"
  logging_dir: "./logs/coco"
  cache_dir: "./cache"
